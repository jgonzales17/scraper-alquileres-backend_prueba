# -*- coding: utf-8 -*-
"""
Scraper completo: Nestoria, Infocasas, Urbania, Properati, Doomos
Filtros opcionales: zona, dormitorios, ba√±os, price_min, price_max, palabras_clave
Salida: DataFrame combinado (mostrado) + CSV (combined_anuncios_filtrados.csv)
"""
import re
import time
import os
import requests
import pandas as pd
from typing import Optional
from bs4 import BeautifulSoup
import logging
from datetime import datetime
import uuid

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Selenium (solo para otros scrapers, no para Nestoria)
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager

COMMON_UA = ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
             "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36")

# -------------------- Helpers --------------------
def create_driver(headless: bool = True):
    options = Options()
    if headless:
        options.add_argument("--headless=new")
    options.add_argument(f"user-agent={COMMON_UA}")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--window-size=1920,1080")
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    try:
        driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
            "source": "Object.defineProperty(navigator, 'webdriver', {get: () => undefined});"
        })
    except Exception:
        pass
    return driver

def slugify_zone(zona: str) -> str:
    if not zona:
        return ""
    s = zona.lower().strip()
    # Reemplazar caracteres especiales y tildes
    trans = str.maketrans("√°√©√≠√≥√∫√±√º", "aeiounu")
    s = s.translate(trans)
    s = re.sub(r"\s+", "-", s)
    s = re.sub(r"[^a-z0-9\-]", "", s)
    return s

def parse_precio_con_moneda(precio_str):
    if not precio_str:
        return (None, None)
    s = str(precio_str)
    moneda = None
    if "S/" in s or s.strip().startswith("S/"):
        moneda = "S"
    elif "$" in s:
        moneda = "USD"
    nums = re.sub(r"[^\d]", "", s)
    return (moneda, int(nums)) if nums else (moneda, None)

def _extract_m2(s):
    if s is None:
        return None
    m = re.search(r"(\d{1,4})\s*(m¬≤|m2)", str(s), flags=re.I)
    return int(m.group(1)) if m else None

def _parse_price_soles(s):
    moneda, val = parse_precio_con_moneda(str(s))
    return val if moneda == "S" else None

# -------------------- Nestoria (VERS√ìN CORREGIDA Y FUNCIONAL CON IM√ÅGENES) --------------------
EXCEPCIONES = ["miraflores", "tarapoto", "la molina", "magdalena", "lambayeque", "ventanilla", "la victoria"]
def normalize_text(text):
    """Elimina acentos y pasa a min√∫sculas"""
    import unicodedata
    return unicodedata.normalize('NFKD', text.lower()).encode('ASCII','ignore').decode('utf-8')

def build_zona_slug_nestoria(zona_input: str) -> str:
    if not zona_input or not zona_input.strip():
        return "lima"  # ‚Üê ¬°ESTO ES LO √öNICO QUE CAMBIA!
    z = zona_input.strip().lower().replace(" ", "-")
    if z not in [e.lower() for e in EXCEPCIONES]:
        return z
    else:
        return "lima_" + z

def _extract_int_from_text(s):
    """
    Extrae el primer n√∫mero entero de una cadena de texto.
    Es m√°s robusta y maneja espacios, saltos de l√≠nea y caracteres especiales.
    """
    if s is None:
        return None
    # Convertir a string y limpiar espacios en blanco alrededor
    text = str(s).strip()
    # Reemplazar cualquier espacio en blanco (incluyendo &nbsp;, tabulaciones, saltos de l√≠nea) por un espacio normal
    text = re.sub(r'\s+', ' ', text)
    # Buscar el primer n√∫mero entero
    m = re.search(r'(\d+)', text)
    return int(m.group(1)) if m else None

def scrape_nestoria(zona: str = "", dormitorios: str = "0", banos: str = "0",
                    price_min: Optional[int] = None, price_max: Optional[int] = None,
                    palabras_clave: str = "", max_results_per_zone: int = 200):
    """
    Scraper FINAL para Nestoria. Usa Selenium.
    Extrae la imagen DEL DETALLE de cada anuncio.
    Solo entra al detalle para obtener la imagen, no para extraer m√°s datos.
    """
    zona_slug = build_zona_slug_nestoria(zona)
    base_url = f"https://www.nestoria.pe/{zona_slug}/inmuebles/alquiler"
    if dormitorios and dormitorios != "0":
        base_url += f"/dormitorios-{dormitorios}"
    params = []
    if banos and banos != "0":
        params.append(f"bathrooms={banos}")
    if price_min and str(price_min) != "0":
        params.append(f"price_min={price_min}")
    if price_max and str(price_max) != "0":
        params.append(f"price_max={price_max}")
    if params:
        base_url += "?" + "&".join(params)
    logger.info(f"URL de Nestoria: {base_url}")
    driver = create_driver(headless=True)
    results = []
    try:
        driver.get(base_url)
        time.sleep(3)
        # Scroll para cargar m√°s resultados
        for _ in range(5):
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(1)
        soup = BeautifulSoup(driver.page_source, "html.parser")
        # Seleccionar los contenedores de anuncios
        items = soup.select("li.rating__new") or soup.select("ul#main__listing_res > li")
        if not items:
            items = [li for li in soup.find_all("li") if li.select_one(".result__details__price")]
        if not items:
            items = soup.find_all(["li", "div", "article"], class_=lambda x: x and any(cls in x for cls in ["listing", "result", "property", "item"]))
        seen_links = set()
        for i, li in enumerate(items):
            try:
                # Extraer link
                a_tag = li.select_one("a.results__link") or li.select_one("a[href]")
                if not a_tag:
                    continue
                link = a_tag.get("data-href") or a_tag.get("href") or ""
                if link and link.startswith("/"):
                    link = "https://www.nestoria.pe" + link
                if not link or link in seen_links:
                    continue
                # Extraer t√≠tulo
                title_elem = li.select_one(".listing__title__text") or li.select_one(".listing__title") or a_tag
                title = title_elem.get_text(" ", strip=True) if title_elem else a_tag.get_text(" ", strip=True)[:140]
                # Extraer precio
                price_elem = li.select_one(".result__details__price span") or li.select_one(".result__details__price") or li.select_one(".price")
                price_text = price_elem.get_text(" ", strip=True) if price_elem else ""
                # Aplicar filtro de precio aqu√≠ mismo
                moneda, precio_val = parse_precio_con_moneda(price_text)
                if price_max is not None and moneda == "S" and precio_val is not None and precio_val > price_max:
                    continue
                if price_min is not None and moneda == "S" and precio_val is not None and precio_val < price_min:
                    continue
                if moneda == "USD" and (price_max is not None or price_min is not None):
                    continue
                # Extraer descripci√≥n
                desc_elem = li.select_one(".listing__description") or li.select_one(".result__summary") or None
                desc = desc_elem.get_text(" ", strip=True) if desc_elem else li.get_text(" ", strip=True)[:800]
                # Extraer dormitorios, ba√±os y m2 del texto
                text_content = li.get_text(" ", strip=True).lower()
                dormitorios_text = ""
                dorm_match = re.search(r'(\d+)\s*dormitori', text_content, flags=re.I)
                if dorm_match:
                    dormitorios_text = dorm_match.group(1)
                banos_text = ""
                banos_match = re.search(r'(\d+)\s*ba√±', text_content, flags=re.I)
                if banos_match:
                    banos_text = banos_match.group(1)
                m2_text = ""
                m2_match = re.search(r'(\d{1,4})\s*(m¬≤|m2)', text_content, flags=re.I)
                if m2_match:
                    m2_text = m2_match.group(1)
                # AHORA: Entrar al detalle para obtener la imagen principal
                img_url = ""
                try:
                    driver.get(link)
                    time.sleep(1)  # Esperar a que cargue la imagen
                    detail_soup = BeautifulSoup(driver.page_source, "html.parser")
                    # Buscar la imagen principal en el detalle
                    main_img = detail_soup.select_one("img[data-element='main-swiper-slide']")
                    if main_img:
                        img_url = main_img.get("src") or main_img.get("data-src") or ""
                        if img_url and img_url.startswith("//"):
                            img_url = "https:" + img_url
                        img_url = img_url.strip()
                    else:
                        # Fallback: buscar cualquier img dentro de .photos .swiper-slide
                        fallback_img = detail_soup.select_one(".photos .swiper-slide img")
                        if fallback_img:
                            img_url = fallback_img.get("src") or fallback_img.get("data-src") or ""
                            if img_url and img_url.startswith("//"):
                                img_url = "https:" + img_url
                            img_url = img_url.strip()
                except Exception as e:
                    logger.error(f"Error al obtener imagen de detalle en Nestoria para {link}: {e}")
                    pass
                results.append({
                    "titulo": title,
                    "precio": price_text,
                    "m2": m2_text,
                    "dormitorios": dormitorios_text,
                    "ba√±os": banos_text,
                    "descripcion": desc,
                    "link": link,
                    "fuente": "nestoria",
                    "imagen_url": img_url,
                    "scraped_at": datetime.now().isoformat(),
                    "id": str(uuid.uuid4())
                })
                seen_links.add(link)
            except Exception as e:
                logger.error(f"Error procesando anuncio en Nestoria: {e}")
                continue
    except Exception as e:
        logger.error(f"Error en Nestoria scraper: {e}")
    finally:
        try:
            driver.quit()
        except:
            pass
    logger.info(f"Procesados {len(results)} anuncios v√°lidos de Nestoria")
    return pd.DataFrame(results)

# -------------------- Infocasas --------------------
def scrape_infocasas(zona: str = "", dormitorios: str = "0", banos: str = "0",
                     price_min: Optional[int] = None, price_max: Optional[int] = None,
                     palabras_clave: str = "", max_scrolls: int = 8):
    # Mapeo espec√≠fico para InfoCasas
    ZONA_MAPEO_INFOCASAS = {
        "anc√≥n": "ancon",
        "ate": "ate",
        "barranco": "barranco",
        "bre√±a": "bre√±a",
        "carabayllo": "carabayllo",
        "chaclacayo": "chaclacayo",
        "chorrillos": "chorrillos",
        "cieneguilla": "cieneguilla",
        "comas": "comas",
        "el agustino": "el-agustino",
        "independencia": "independencia",
        "jes√∫s mar√≠a": "jesus-maria",
        "la molina": "la-molina",
        "la victoria": "la-victoria",
        "lima": "lima-cercado",
        "lince": "lince",
        "los olivos": "los-olivos",
        "lurigancho": "lurigancho",
        "lur√≠n": "lurin",
        "magdalena del mar": "magdalena-del-mar",
        "miraflores": "miraflores",
        "pachac√°mac": "pachacamac",
        "pucusana": "pucusana",
        "pueblo libre": "pueblo-libre",
        "puente piedra": "puente-piedra",
        "punta hermosa": "punta-hermosa",
        "punta negra": "punta-negra",
        "r√≠mac": "rimac",
        "san bartolo": "san-bartolo",
        "san borja": "san-borja",
        "san isidro": "san-isidro",
        "san juan de lurigancho": "san-juan-de-lurigancho",
        "san juan de miraflores": "san-juan-de-miraflores",
        "san luis": "san-luis",
        "san mart√≠n de porres": "san-martin-de-porres",
        "san miguel": "san-miguel",
        "santa anita": "santa-anita",
        "santa mar√≠a del mar": "santa-maria-del-mar",
        "santa rosa": "santa-rosa",
        "santiago de surco": "santiago-de-surco",
        "surquillo": "surquillo",
        "villa el salvador": "villa-el-salvador",
        "villa mar√≠a del triunfo": "villa-maria-del-triunfo"
    }
    # Construir URL base seg√∫n la zona
    if zona and zona.strip():
        zona_lower = zona.strip().lower()
        zone_slug = ZONA_MAPEO_INFOCASAS.get(zona_lower, slugify_zone(zona))
        base = f"https://www.infocasas.com.pe/alquiler/casas-y-departamentos/lima/{zone_slug}"
    else:
        base = "https://www.infocasas.com.pe/alquiler/casas-y-departamentos"
    # Agregar filtros si est√°n especificados
    if dormitorios and dormitorios != "0" and banos and banos != "0" and price_min is not None and price_max is not None:
        base += f"/{dormitorios}-dormitorio/{banos}-bano/desde-{price_min}/hasta-{price_max}?&IDmoneda=6"
    elif dormitorios and dormitorios != "0" and banos and banos != "0":
        base += f"/{dormitorios}-dormitorio/{banos}-bano"
    elif dormitorios and dormitorios != "0":
        base += f"/{dormitorios}-dormitorio"
    elif banos and banos != "0":
        base += f"/{banos}-bano"
    # Agregar par√°metros de b√∫squeda si existen
    if palabras_clave and palabras_clave.strip():
        if "?" in base:
            base += f"&searchstring={requests.utils.quote(palabras_clave.strip())}"
        else:
            base += f"?searchstring={requests.utils.quote(palabras_clave.strip())}"
    logger.info(f"URL de InfoCasas: {base}")
    driver = create_driver(headless=True)
    results = []
    try:
        driver.get(base)
        time.sleep(2)  # Esperar a que cargue la p√°gina
        # Hacer scroll para cargar m√°s resultados
        for _ in range(max_scrolls):
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(0.6)
        soup = BeautifulSoup(driver.page_source, "html.parser")
        # Buscar los contenedores de anuncios espec√≠ficos de InfoCasas
        nodes = soup.select("div.listingCard") or soup.select("article")
        for n in nodes:
            try:
                # Verificar que el elemento tiene el atributo href
                a = n.select_one("a[href]")
                if not a:
                    continue
                href = a.get("href") if a else ""
                # Construir URL completa
                if href and href.startswith("/"):
                    href = "https://www.infocasas.com.pe" + href
                # Extraer t√≠tulo
                title_elem = n.select_one("h2.lc-title") or n.select_one(".lc-title") or a
                title = title_elem.get_text(" ", strip=True) if title_elem else n.get_text(" ", strip=True)[:250]
                # Extraer precio
                price = ""
                price_elem = n.select_one(".main-price") or n.select_one(".lc-price p") or n.select_one(".property-price-tag p")
                if price_elem:
                    price = price_elem.get_text(" ", strip=True)
                # Extraer ubicaci√≥n
                location_elem = n.select_one(".lc-location") or n.select_one("strong")
                location = location_elem.get_text(" ", strip=True) if location_elem else ""
                # Extraer dormitorios, ba√±os y m¬≤ de los tags
                dormitorios_text = ""
                banos_text = ""
                m2_text = ""
                # Buscar en los elementos con clase lc-typologyTag__item
                typology_items = n.select(".lc-typologyTag__item strong")
                for item in typology_items:
                    text = item.get_text().strip()
                    if "Dorm" in text:
                        dorm_match = re.search(r'(\d+)', text)
                        if dorm_match:
                            dormitorios_text = dorm_match.group(1)
                    elif "Ba√±os" in text or "Ba√±o" in text:
                        banos_match = re.search(r'(\d+)', text)
                        if banos_match:
                            banos_text = banos_match.group(1)
                    elif "m¬≤" in text:
                        m2_match = re.search(r'(\d+)', text)
                        if m2_match:
                            m2_text = m2_match.group(1)
                # Extraer descripci√≥n
                desc_elem = n.select_one(".lc-description") or n.select_one("p")
                desc = desc_elem.get_text(" ", strip=True) if desc_elem else n.get_text(" ", strip=True)[:400]
                # EXTRAER IMAGEN DIRECTAMENTE DEL LISTADO (NO ENTRAR AL DETALLE)
                img_url = ""
                img_tag = n.select_one(".cardImageGallery .gallery-image img")
                if img_tag:
                    img_url = img_tag.get("src") or img_tag.get("data-src") or ""
                    if img_url and img_url.startswith("//"):
                        img_url = "https:" + img_url
                    img_url = img_url.strip()
                results.append({
                    "titulo": title, 
                    "precio": price, 
                    "m2": m2_text,
                    "dormitorios": dormitorios_text, 
                    "ba√±os": banos_text, 
                    "descripcion": desc,
                    "link": href or "", 
                    "fuente": "infocasas",
                    "imagen_url": img_url,
                    "scraped_at": datetime.now().isoformat(),
                    "id": str(uuid.uuid4())
                })
            except Exception as e:
                logger.error(f"Error procesando anuncio en InfoCasas: {e}")
                continue
    except Exception as e:
        logger.error(f"Error en InfoCasas scraper: {e}")
        pass
    finally:
        try:
            driver.quit()
        except:
            pass
    return pd.DataFrame(results)

# -------------------- Urbania --------------------
def scrape_urbania(zona: str = "", dormitorios: str = "0", banos: str = "0",
                   price_min: Optional[int] = None, price_max: Optional[int] = None,
                   palabras_clave: str = "", max_pages: int = 6, wait_time: float = 1.5):
    zona = (zona or "").strip()
    # construir keyword combinando filtros (si el usuario solo pone keyword, la usamos)
    kw_parts = []
    if palabras_clave and palabras_clave.strip():
        kw_parts.append(palabras_clave.strip())
    if dormitorios and str(dormitorios) != "0":
        kw_parts.append(f"{dormitorios} dormitorios")
    if banos and str(banos) != "0":
        kw_parts.append(f"{banos} banos")
    keyword_value = " ".join(kw_parts).strip()
    # CAMBIO CLAVE: Siempre usar la zona si est√° especificada, independientemente de las keywords
    if zona:
        # Mapeo espec√≠fico para Urbania
        ZONA_MAPEO_URBANIA = {
            "anc√≥n": "ancon",
            "ate": "ate-vitarte",  # Usar ate-vitarte como fallback
            "barranco": "barranco",
            "bre√±a": "brena",
            "carabayllo": "carabayllo",
            "chaclacayo": "chaclacayo",
            "chorrillos": "chorrillos",
            "cieneguilla": "cieneguilla",
            "comas": "comas",
            "el agustino": "el-agustino",
            "independencia": "independencia",
            "jes√∫s mar√≠a": "jesus-maria",
            "la molina": "la-molina",
            "la victoria": "la-victoria",
            "lima": "lima-cercado",
            "lince": "lince",
            "los olivos": "los-olivos",
            "lurigancho": "lurigancho",
            "lur√≠n": "lurin",
            "magdalena del mar": "magdalena-del-mar",
            "miraflores": "miraflores",
            "pachac√°mac": "pachacamac",
            "pucusana": "pucusana",
            "pueblo libre": "pueblo-libre",
            "puente piedra": "puente-piedra",
            "punta hermosa": "punta-hermosa",
            "punta negra": "punta-negra",
            "r√≠mac": "rimac",
            "san bartolo": "san-bartolo",
            "san borja": "san-borja",
            "san isidro": "san-isidro",
            "san juan de lurigancho": "san-juan-de-lurigancho",
            "san juan de miraflores": "san-juan-de-miraflores",
            "san luis": "san-luis",
            "san mart√≠n de porres": "san-martin-de-porres",
            "san miguel": "san-miguel",
            "santa anita": "santa-anita",
            "santa mar√≠a del mar": "santa-maria-del-mar",
            "santa rosa": "santa-rosa",
            "santiago de surco": "santiago-de-surco",
            "surquillo": "surquillo",
            "villa el salvador": "villa-el-salvador",
            "villa mar√≠a del triunfo": "villa-maria-del-triunfo"
        }
        zona_lower = zona.strip().lower()
        zone_slug = ZONA_MAPEO_URBANIA.get(zona_lower, slugify_zone(zona))
        base = f"https://urbania.pe/buscar/alquiler-de-departamentos-en-{zone_slug}--lima--lima"
    else:
        base = "https://urbania.pe/buscar/alquiler-de-departamentos"
    params = []
    if keyword_value:
        params.append(f"keyword={requests.utils.quote(keyword_value)}")
    if price_min is not None:
        params.append(f"priceMin={price_min}")
    if price_max is not None:
        params.append(f"priceMax={price_max}")
    if dormitorios and dormitorios != "0":
        params.append(f"bedroomMin={dormitorios}")
    if banos and banos != "0":
        params.append(f"bathroomMin={banos}")
    if price_min is not None or price_max is not None:
        params.append("currencyId=6")  # Soles
    url = base + ("?" + "&".join(params) if params else "")
    logger.info(f"URL de Urbania: {url}")
    driver = create_driver(headless=True)
    results = []
    seen = set()
    try:
        driver.get(url)
        # esperar unos segundos por elementos representativos (no bloquear si timeout)
        try:
            WebDriverWait(driver, 12).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "article, div[data-qa='posting PROPERTY'], div.postingCard"))
            )
        except:
            pass
        page_count = 0
        while page_count < max_pages:
            page_count += 1
            last_h = driver.execute_script("return document.body.scrollHeight")
            for _ in range(8):
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(wait_time)
                new_h = driver.execute_script("return document.body.scrollHeight")
                if new_h == last_h:
                    break
                last_h = new_h
            soup = BeautifulSoup(driver.page_source, "html.parser")
            # intentar varios selectores
            card_selectors = [
                "div[data-qa='posting PROPERTY']",
                "article",
                "div.postingCard-module__posting",
                "div.postingCard",
                "div.posting-card",
                "div[class*='postingCard']",
            ]
            cards = []
            for sel in card_selectors:
                found = soup.select(sel)
                if found and len(found) > 0:
                    cards = found
                    break
            if not cards:
                cards = soup.select("a[href]")[:0]  # vac√≠o
            prev_len = len(results)
            for c in cards:
                try:
                    a_tag = c.select_one("a[href]") or c.select_one("h2 a") or c.select_one("h3 a")
                    link = a_tag.get("href") if a_tag else ""
                    if link and link.startswith("/"):
                        link = "https://urbania.pe" + link
                    if not link:
                        continue
                    if link in seen:
                        continue
                    seen.add(link)
                    title = a_tag.get_text(" ", strip=True) if a_tag and a_tag.get_text(strip=True) else (c.get_text(" ", strip=True)[:140])
                    price_el = c.select_one("div.postingPrices-module__price") or c.select_one(".first-price") or c.select_one(".price")
                    price = price_el.get_text(" ", strip=True) if price_el else ""
                    desc = c.get_text(" ", strip=True)[:400]
                    img = ""
                    img_tag = c.select_one("img")
                    if img_tag:
                        img = img_tag.get("src") or img_tag.get("data-src") or ""
                        if img and img.startswith("//"): img = "https:" + img
                        # Limpiar espacios al final
                        img = img.strip()
                    # EXTRAER DORMITORIOS
                    dormitorios_text = ""
                    dorm_elem = c.select_one(".postingMainFeatures-module__posting-main-features-span:contains('dorm.')")
                    if dorm_elem:
                        dorm_text = dorm_elem.get_text(" ", strip=True)
                        dorm_match = re.search(r'(\d+)', dorm_text)
                        if dorm_match:
                            dormitorios_text = dorm_match.group(1)
                    # EXTRAER BA√ëOS
                    banos_text = ""
                    banos_elem = c.select_one(".postingMainFeatures-module__posting-main-features-span:contains('ba√±o')")
                    if banos_elem:
                        banos_text_full = banos_elem.get_text(" ", strip=True)
                        banos_match = re.search(r'(\d+)', banos_text_full)
                        if banos_match:
                            banos_text = banos_match.group(1)
                    # EXTRAER METROS CUADRADOS
                    m2_text = ""
                    m2_elem = c.select_one(".postingMainFeatures-module__posting-main-features-span:contains('m¬≤')")
                    if m2_elem:
                        m2_text_full = m2_elem.get_text(" ", strip=True)
                        m2_match = re.search(r'(\d+)', m2_text_full)
                        if m2_match:
                            m2_text = m2_match.group(1)
                    # AHORA INCLUIMOS LOS VALORES EXTRA√çDOS
                    results.append({
                        "titulo": title, 
                        "precio": price, 
                        "m2": m2_text,
                        "dormitorios": dormitorios_text, 
                        "ba√±os": banos_text,
                        "descripcion": desc, 
                        "link": link, 
                        "fuente": "urbania",
                        "imagen_url": img,
                        "scraped_at": datetime.now().isoformat(),
                        "id": str(uuid.uuid4())
                    })
                except Exception as e:
                    logger.error(f"Error procesando anuncio en Urbania: {e}")
                    continue
            # si no hay nuevos resultados intentar paginar/click "cargar m√°s"
            if len(results) == prev_len:
                clicked = False
                try:
                    # probar varios selectores para "cargar m√°s" / siguiente
                    next_selectors = [
                        "a[rel='next']", "a[aria-label='Siguiente']", "a[data-qa='pagination-next']",
                        "button[data-qa='pagination-next']", "a.pagination__next", "a.next", "button.load-more", "a.load-more"
                    ]
                    for sel in next_selectors:
                        elems = driver.find_elements(By.CSS_SELECTOR, sel)
                        for e in elems:
                            try:
                                if e.is_displayed():
                                    driver.execute_script("arguments[0].scrollIntoView(true);", e)
                                    time.sleep(0.2)
                                    e.click()
                                    time.sleep(wait_time + 0.5)
                                    clicked = True
                                    break
                            except:
                                continue
                        if clicked:
                            break
                except:
                    clicked = False
                if not clicked:
                    # intentar incrementar page= en URL
                    cur = driver.current_url
                    m = re.search(r"([?&]page=)(\d+)", cur)
                    if m:
                        cur_page = int(m.group(2))
                        next_page = cur_page + 1
                        new_url = re.sub(r"([?&]page=)\d+", r"\1{}".format(next_page), cur)
                        try:
                            driver.get(new_url)
                            time.sleep(wait_time + 0.8)
                            clicked = True
                        except:
                            clicked = False
                if not clicked:
                    break
            time.sleep(0.4)
        return pd.DataFrame(results)
    except Exception as e:
        logger.error(f"Error en Urbania scraper: {e}")
        return pd.DataFrame()
    finally:
        try:
            driver.quit()
        except:
            pass

# -------------------- Properati --------------------
def scrape_properati(zona: str = "", dormitorios: str = "0", banos: str = "0",
                     price_min: Optional[int] = None, price_max: Optional[int] = None,
                     palabras_clave: str = ""):
    if zona and zona.strip():
        # Mapeo espec√≠fico para Properati
        ZONA_MAPEO_PROPERATI = {
            "anc√≥n": "ancon",
            "ate": "ate",
            "barranco": "barranco",
            "bre√±a": "brena",
            "carabayllo": "carabayllo",
            "chaclacayo": "chaclacayo",
            "chorrillos": "chorrillos",
            "cieneguilla": "cieneguilla",
            "comas": "comas",
            "el agustino": "el-agustino",
            "independencia": "independencia",
            "jes√∫s mar√≠a": "jesus-maria",
            "la molina": "la-molina",
            "la victoria": "la-victoria",
            "lima": "lima",
            "lince": "lince",
            "los olivos": "los-olivos",
            "lurigancho": "lurigancho",
            "lur√≠n": "lurin",
            "magdalena del mar": "magdalena-del-mar",
            "miraflores": "miraflores",
            "pachac√°mac": "pachacamac",
            "pucusana": "pucusana",
            "pueblo libre": "pueblo-libre",
            "puente piedra": "puente-piedra",
            "punta hermosa": "punta-hermosa",
            "punta negra": "punta-negra",
            "r√≠mac": "rimac",
            "san bartolo": "san-bartolo",
            "san borja": "san-borja",
            "san isidro": "san-isidro",
            "san juan de lurigancho": "san-juan-de-lurigancho",
            "san juan de miraflores": "san-juan-de-miraflores",
            "san luis": "san-luis",
            "san mart√≠n de porres": "san-martin-de-porres",
            "san miguel": "san-miguel",
            "santa anita": "santa-anita",
            "santa mar√≠a del mar": "santa-maria-del-mar",
            "santa rosa": "santa-rosa",
            "santiago de surco": "santiago-de-surco",
            "surquillo": "surquillo",
            "villa el salvador": "villa-el-salvador",
            "villa mar√≠a del triunfo": "villa-maria-del-triunfo"
        }
        zona_lower = zona.strip().lower()
        zone_slug = ZONA_MAPEO_PROPERATI.get(zona_lower, slugify_zone(zona))
        base = f"https://www.properati.com.pe/s/{zone_slug}/alquiler?propertyType=apartment%2Chouse"
    else:
        base = "https://www.properati.com.pe/s/alquiler?propertyType=apartment%2Chouse"
    # Agregar par√°metros de filtros
    params = []
    if dormitorios and dormitorios != "0":
        params.append(f"bedrooms={dormitorios}")
    if banos and banos != "0":
        params.append(f"bathrooms={banos}")
    if price_min is not None:
        params.append(f"minPrice={price_min}")
    if price_max is not None:
        params.append(f"maxPrice={price_max}")
    # Procesar palabras clave: convertir "piscina" ‚Üí amenities=swimming_pool, "jardin" ‚Üí amenities=garden
    if palabras_clave and palabras_clave.strip():
        palabras = palabras_clave.lower().split()
        amenities = []
        other_keywords = []
        for p in palabras:
            if p == "piscina":
                amenities.append("swimming_pool")
            elif p == "jardin":
                amenities.append("garden")
            else:
                other_keywords.append(p)
        # Si hay amenities, usarlas como par√°metro separado
        if amenities:
            base += "&amenities=" + ",".join(amenities)
        # Si quedan otras palabras clave, agregarlas como keyword
        if other_keywords:
            base += "&keyword=" + requests.utils.quote(" ".join(other_keywords))
    # Construir URL final
    if params:
        base += "&" + "&".join(params)
    logger.info(f"URL de Properati: {base}")
    try:
        r = requests.get(base, headers={"User-Agent": COMMON_UA}, timeout=15)
        r.raise_for_status()
    except:
        return pd.DataFrame()
    soup = BeautifulSoup(r.text, "html.parser")
    cards = soup.select("article") or soup.select("div.posting-card") or soup.select("a[href]")
    results = []
    for c in cards:
        try:
            a = c.select_one("a[href]") or c.select_one("a.title")
            href = a.get("href") if a else ""
            if href and href.startswith("/"):
                href = "https://www.properati.com.pe" + href
            title = a.get_text(" ", strip=True) if a else c.get_text(" ", strip=True)[:140]
            price = ""
            price_elem = c.select_one(".price")
            if price_elem:
                price = price_elem.get_text(" ", strip=True)
            # EXTRAER DORMITORIOS
            dormitorios_text = ""
            dorm_elem = c.select_one(".properties__bedrooms")
            if dorm_elem:
                dorm_text = dorm_elem.get_text(" ", strip=True)
                dorm_match = re.search(r'(\d+)', dorm_text)
                if dorm_match:
                    dormitorios_text = dorm_match.group(1)
            # EXTRAER BA√ëOS
            banos_text = ""
            banos_elem = c.select_one(".properties__bathrooms")
            if banos_elem:
                banos_text_full = banos_elem.get_text(" ", strip=True)
                banos_match = re.search(r'(\d+)', banos_text_full)
                if banos_match:
                    banos_text = banos_match.group(1)
            # EXTRAER METROS CUADRADOS
            m2_text = ""
            m2_elem = c.select_one(".properties__area")
            if m2_elem:
                m2_text_full = m2_elem.get_text(" ", strip=True)
                m2_match = re.search(r'(\d+)', m2_text_full)
                if m2_match:
                    m2_text = m2_match.group(1)
            img = ""
            img_tag = c.select_one("img")
            if img_tag:
                img = img_tag.get("src") or img_tag.get("data-src") or ""
                # Filtrar im√°genes no deseadas: solo aceptar las que comienzan con https://img (no con https://images.proppit)
                if img and img.startswith("https://img"):
                    img = img.strip()
                elif img and img.startswith("//"):
                    img_full = "https:" + img
                    if img_full.startswith("https://img"):
                        img = img_full.strip()
                    else:
                        img = ""  # Rechazar otras fuentes
                else:
                    img = ""  # Rechazar si no cumple con el criterio
            # AHORA INCLUIMOS LOS VALORES EXTRA√çDOS
            results.append({
                "titulo": title, 
                "precio": price, 
                "m2": m2_text,
                "dormitorios": dormitorios_text, 
                "ba√±os": banos_text,
                "descripcion": title, 
                "link": href or "", 
                "fuente": "properati",
                "imagen_url": img,
                "scraped_at": datetime.now().isoformat(),
                "id": str(uuid.uuid4())
            })
        except Exception as e:
            logger.error(f"Error en Properati al procesar un anuncio: {e}")
            continue
    return pd.DataFrame(results)

# -------------------- Doomos --------------------
def scrape_doomos(zona: str = "", dormitorios: str = "0", banos: str = "0",
                  price_min: Optional[int] = None, price_max: Optional[int] = None,
                  palabras_clave: str = ""):
    driver = create_driver(headless=True)
    results = []
    try:
        # Mapeo ACTUALIZADO de zonas a sus IDs espec√≠ficos para Doomos
        ZONA_IDS_CORRECTOS = {
            "anc√≥n": "-336912",
            "ate": "-337679",
            "bre√±a": "65645345",
            "carabayllo": "-339907",
            "chaclacayo": "-341190",
            "chorrillos": "-342811",
            "cieneguilla": "-343329",
            "comas": "-343903",
            "el agustino": "-345552",
            "jes√∫s mar√≠a": "348294",
            "la molina": "-351740",
            "la victoria": "-352442",
            "lima": "45343445",  # Cercado de Lima
            "lince": "-352696",
            "los olivos": "191126",
            "lurigancho": "-353648",
            "lur√≠n": "-353652",
            "magdalena del mar": "326245",
            "miraflores": "-354864",
            "pachac√°mac": "-356636",
            "pucusana": "-359672",
            "pueblo libre": "-359690",
            "puente piedra": "-359759",
            "punta hermosa": "-360186",
            "punta negra": "-360189",
            "r√≠mac": "-361308",
            "san bartolo": "-362154",
            "san borja": "-362170",
            "san isidro": "-362425",
            "san luis": "-362738",
            "san miguel": "-362804",
            "santiago de surco": "-364705",
            "surquillo": "-364723"
        }
        # Construir URL base CORRECTA para Doomos
        base_url = "http://www.doomos.com.pe/search/"
        # Par√°metros base
        params = {
            "clase": "1",           # Departamentos
            "stipo": "16",          # Alquiler
            "pagina": "1",
            "sort": "primeasc"
        }
        # Si NO se especifica zona, usar LIMA por defecto con el ID CORRECTO
        if not zona or not zona.strip():
            params["loc_name"] = "Lima (Regi√≥n de Lima)"
            params["loc_id"] = "-352647"  # ‚Üê ¬°¬°¬°ESTA ES LA L√çNEA CORREGIDA!!!
        else:
            zona_lower = zona.strip().lower()
            loc_id = ZONA_IDS_CORRECTOS.get(zona_lower, "")
            zona_formateada = f"{zona.strip()} (Regi√≥n de Lima)"
            params["loc_name"] = zona_formateada
            if loc_id:
                params["loc_id"] = loc_id
        # Agregar filtros opcionales
        if dormitorios and dormitorios != "0":
            params["piezas"] = dormitorios
        if banos and banos != "0":
            params["banos"] = banos
        if price_min is not None:
            params["preciomin"] = str(price_min)
        if price_max is not None:
            params["preciomax"] = str(price_max)
        if palabras_clave and palabras_clave.strip():
            params["keyword"] = palabras_clave.strip()
        # Construir URL completa
        url = base_url + "?" + "&".join(f"{k}={requests.utils.quote(str(v))}" for k,v in params.items())
        logger.info(f"URL de Doomos: {url}")
        driver.get(url)
        time.sleep(3)
        # Scroll para cargar m√°s resultados
        for _ in range(3):
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(1)
        soup = BeautifulSoup(driver.page_source, "html.parser")
        cards = soup.select(".content_result")
        if not cards:
            logger.warning("No se encontraron cards en Doomos")
            return pd.DataFrame()
        logger.info(f"Se encontraron {len(cards)} cards en Doomos")
        for card in cards:
            try:
                # Extraer link y t√≠tulo
                a_tag = card.select_one(".content_result_titulo a")
                if not a_tag:
                    continue
                title = a_tag.get_text(" ", strip=True)
                href = a_tag.get("href") or ""
                # Construir URL completa si es relativa
                if href and href.startswith("/"):
                    href = "http://www.doomos.com.pe" + href
                # Extraer precio
                price_elem = card.select_one(".content_result_precio")
                price = price_elem.get_text(" ", strip=True) if price_elem else ""
                # Extraer descripci√≥n
                desc_elem = card.select_one(".content_result_descripcion")
                desc = desc_elem.get_text(" ", strip=True) if desc_elem else card.get_text(" ", strip=True)[:400]
                # Extraer dormitorios, ba√±os, m2 del texto
                dormitorios_text = ""
                banos_text = ""
                m2_text = ""
                text_content = card.get_text(" ", strip=True).lower()
                dorm_match = re.search(r'(\d+)\s*dormitorio', text_content)
                if dorm_match:
                    dormitorios_text = dorm_match.group(1)
                banos_match = re.search(r'(\d+)\s*ba√±o', text_content)
                if banos_match:
                    banos_text = banos_match.group(1)
                m2_match = re.search(r'(\d+)\s*m2', text_content)
                if m2_match:
                    m2_text = m2_match.group(1)
                # EXTRAER IMAGEN DIRECTAMENTE DEL LISTADO (NO ENTRAR AL DETALLE)
                img_url = ""
                img_tag = card.select_one("img.content_result_image")
                if img_tag:
                    img_url = img_tag.get("src") or img_tag.get("data-src") or ""
                    if img_url and img_url.startswith("//"):
                        img_url = "https:" + img_url
                    img_url = img_url.strip()
                results.append({
                    "titulo": title,
                    "precio": price,
                    "m2": m2_text,
                    "dormitorios": dormitorios_text,
                    "ba√±os": banos_text,
                    "descripcion": desc,
                    "link": href,
                    "fuente": "doomos",
                    "imagen_url": img_url,
                    "scraped_at": datetime.now().isoformat(),
                    "id": str(uuid.uuid4())
                })
            except Exception as e:
                logger.error(f"Error procesando card en Doomos: {e}")
                continue
    except Exception as e:
        logger.error(f"Error en Doomos scraper: {e}")
    finally:
        try:
            driver.quit()
        except:
            pass
    return pd.DataFrame(results)

# -------------------- Filtrado y Unificaci√≥n --------------------
SCRAPERS = [
    ("nestoria", scrape_nestoria),
    ("infocasas", scrape_infocasas),
    ("urbania", scrape_urbania),
    ("properati", scrape_properati),
    ("doomos", scrape_doomos),
]

def _filter_df_strict(df, dormitorios_req, banos_req, price_min, price_max):
    if df is None or df.empty:
        return pd.DataFrame()
    dfc = df.copy().reset_index(drop=True)
    dfc["_precio_soles"] = dfc["precio"].apply(_parse_price_soles)
    dfc["_dorm_num"] = dfc["dormitorios"].apply(_extract_int_from_text)
    dfc["_banos_num"] = dfc["ba√±os"].apply(_extract_int_from_text)
    mask = pd.Series(True, index=dfc.index)
    # only require dorm/banos if user requested them
    try:
        if dormitorios_req is not None and str(dormitorios_req).strip() != "" and str(dormitorios_req) != "0":
            dorm_req_int = int(dormitorios_req)
            mask &= (dfc["_dorm_num"].notnull()) & (dfc["_dorm_num"] == dorm_req_int)
    except:
        pass
    try:
        if banos_req is not None and str(banos_req).strip() != "" and str(banos_req) != "0":
            banos_req_int = int(banos_req)
            mask &= (dfc["_banos_num"].notnull()) & (dfc["_banos_num"] == banos_req_int)
    except:
        pass
    if (price_min is not None) or (price_max is not None):
        if price_min is None:
            price_min = -10**12
        if price_max is None:
            price_max = 10**12
        mask &= dfc["_precio_soles"].notnull()
        mask &= (dfc["_precio_soles"] >= int(price_min)) & (dfc["_precio_soles"] <= int(price_max))
    df_filtered = dfc.loc[mask].copy().reset_index(drop=True)
    df_filtered.drop(columns=["_precio_soles","_dorm_num","_banos_num"], errors="ignore", inplace=True)
    return df_filtered

def _filter_by_keywords(df, palabras_clave: str):
    if df is None or df.empty or not palabras_clave or not palabras_clave.strip():
        return df
    palabras = palabras_clave.lower().split()
    dfc = df.copy()
    dfc["texto_completo"] = (
        dfc["titulo"].astype(str) + " " +
        dfc.get("descripcion", pd.Series([""]*len(dfc))).astype(str) + " " +
        dfc.get("m2", pd.Series([""]*len(dfc))).astype(str) + " " +
        dfc.get("dormitorios", pd.Series([""]*len(dfc))).astype(str) + " " +
        dfc.get("ba√±os", pd.Series([""]*len(dfc))).astype(str)
    ).str.lower()
    for p in palabras:
        dfc = dfc[dfc["texto_completo"].str.contains(re.escape(p), na=False, case=False)]
    dfc.drop(columns=["texto_completo"], errors="ignore", inplace=True)
    return dfc

def run_scrapers(zona: str = "", dormitorios: str = "0", banos: str = "0",
                        price_min: Optional[int] = None, price_max: Optional[int] = None,
                        palabras_clave: str = ""):
    """
    Ejecuta todos los scrapers y devuelve los resultados combinados
    """
    frames = []
    counts = {}
    logger.info(f"üîé Buscando en {zona} | dorms={dormitorios} | ba√±os={banos} | precio={price_min}-{price_max} | palabras_clave='{palabras_clave}'")
    for name, func in SCRAPERS:
        try:
            df = func(zona=zona, dormitorios=dormitorios, banos=banos, price_min=price_min, price_max=price_max, palabras_clave=palabras_clave)
        except TypeError:
            # backward compatibility: call with fewer args
            try:
                df = func(zona, dormitorios, banos, price_min, price_max)
            except Exception as e:
                logger.error(f" ‚ùå Error ejecutando {name} (fallback): {e}")
                df = pd.DataFrame()
        except Exception as e:
            logger.error(f" ‚ùå Error ejecutando {name}: {e}")
            df = pd.DataFrame()
        if df is None or not isinstance(df, pd.DataFrame):
            df = pd.DataFrame(columns=["titulo","precio","m2","dormitorios","ba√±os","descripcion","link","imagen_url","fuente","scraped_at","id"])
        # ensure columns present
        required_columns = ["titulo","precio","m2","dormitorios","ba√±os","descripcion","link","imagen_url","fuente","scraped_at","id"]
        for col in required_columns:
            if col not in df.columns:
                df[col] = ""
        total_raw = len(df)
        counts[name] = total_raw
        logger.info(f"Fuente: {name} -> encontrados: {total_raw}")
        # normalize
        df = df.fillna("").astype(object)
        for col in required_columns:
            df[col] = df[col].astype(str).str.strip().replace({None: "", "None": ""})
        # strict filters (price/dorm/banos)
        df_filtered = _filter_df_strict(df, dormitorios, banos, price_min, price_max)
        logger.info(f"Fuente: {name} -> despu√©s filtrado estricto: {len(df_filtered)}")
        # keywords: apply post-scrape ONLY for sources that didn't use keyword in URL
        # EXCLUDE properati because it uses 'amenities' and text may not contain the keyword
        if palabras_clave and palabras_clave.strip() and name not in ("urbania", "doomos", "properati"):
            prev = len(df_filtered)
            df_filtered = _filter_by_keywords(df_filtered, palabras_clave)
            logger.info(f"Fuente: {name} -> despu√©s filtrar por keywords: {len(df_filtered)} (eliminados {prev - len(df_filtered)})")
        if len(df_filtered) > 0:
            frames.append(df_filtered)
    if not frames:
        logger.warning("‚ö†Ô∏è Ninguna fuente devolvi√≥ anuncios tras filtrar.")
        return pd.DataFrame()
    combined = pd.concat(frames, ignore_index=True, sort=False)
    # Eliminar filas donde el link empieza con "#" o est√° vac√≠o
    combined = combined[~combined["link"].str.startswith("#")].reset_index(drop=True)
    combined = combined[combined["link"] != ""].reset_index(drop=True)
    combined = combined.drop_duplicates(subset=["link","titulo"], keep="first").reset_index(drop=True)
    return combined

# Para uso como m√≥dulo
if __name__ == "__main__":
    # Ejemplo de uso directo
    resultados = run_scrapers("miraflores", "2", "1", 1000, 2000, "piscina")
    print(f"Se encontraron {len(resultados)} propiedades")
    print(resultados.head())